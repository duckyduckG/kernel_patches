From b3876b20a6b3e5ddbc4acc79fee3779a28fa5d5e Mon Sep 17 00:00:00 2001
From: Pavel Dubrova <pashadubrova@gmail.com>
Date: Sun, 12 Dec 2021 05:35:28 +0200
Subject: [PATCH 091/175] staging: ion: Add support for ion CMA heap to
 allocate from carveouts

Because of an XPU errata we need to temporarily have the ION CMA heap
support allocating from a careveout which isn't mapped into the kernel.
Once the errata is fixed this change can be reverted. This patch is
written with reference from commit "a568a844adc" in msm-4.14.

[Pavel Dubrova]
Adapted for 4.19 kernel. Original commit:
https://source.codeaurora.org/quic/la/kernel/msm-5.4/commit/?h=LA.UM.9.16.r1-08500-MANNAR.0&id=b97c60442af50434b493901e956bac63b4d40c04

Change-Id: Id2bbb81644be514bdb90e743a028e65a05ebd55e
Signed-off-by: Liam Mark <lmark@codeaurora.org>
Signed-off-by: Vivek Kumar <vivekuma@codeaurora.org>
Signed-off-by: Zhiqiang Tu <ztu@codeaurora.org>
Signed-off-by: Pavel Dubrova <pashadubrova@gmail.com>
---
 drivers/staging/android/ion/ion_cma_heap.c | 128 ++++++++++++++-------
 1 file changed, 88 insertions(+), 40 deletions(-)

diff --git a/drivers/staging/android/ion/ion_cma_heap.c b/drivers/staging/android/ion/ion_cma_heap.c
index 2591a451e20a..46a8b5a152c5 100644
--- a/drivers/staging/android/ion/ion_cma_heap.c
+++ b/drivers/staging/android/ion/ion_cma_heap.c
@@ -14,6 +14,7 @@
 #include <linux/scatterlist.h>
 #include <soc/qcom/secure_buffer.h>
 #include <linux/highmem.h>
+#include <linux/of.h>
 
 #include "ion.h"
 #include "ion_secure_util.h"
@@ -23,6 +24,12 @@ struct ion_cma_heap {
 	struct cma *cma;
 };
 
+struct ion_cma_buffer_info {
+	void *cpu_addr;
+	dma_addr_t handle;
+	struct page *pages;
+};
+
 #define to_cma_heap(x) container_of(x, struct ion_cma_heap, heap)
 
 static bool ion_heap_is_cma_heap_type(enum ion_heap_type type)
@@ -30,90 +37,137 @@ static bool ion_heap_is_cma_heap_type(enum ion_heap_type type)
 	return type == ION_HEAP_TYPE_DMA;
 }
 
+static bool ion_cma_has_kernel_mapping(struct ion_heap *heap)
+{
+	struct device *dev = heap->priv;
+	struct device_node *mem_region;
+
+	mem_region = of_parse_phandle(dev->of_node, "memory-region", 0);
+	if (!mem_region)
+		return false;
+
+	return !of_property_read_bool(mem_region, "no-map");
+}
+
 /* ION CMA heap operations functions */
 static int ion_cma_allocate(struct ion_heap *heap, struct ion_buffer *buffer,
 			    unsigned long len,
 			    unsigned long flags)
 {
 	struct ion_cma_heap *cma_heap = to_cma_heap(heap);
+	struct ion_cma_buffer_info *info;
 	struct sg_table *table;
-	struct page *pages;
+	struct page *pages = NULL;
 	unsigned long size = PAGE_ALIGN(len);
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 	unsigned long align = get_order(size);
 	int ret;
 	struct device *dev = heap->priv;
 
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info)
+		return -ENOMEM;
+
 	if (ion_heap_is_cma_heap_type(buffer->heap->type) &&
 	    is_secure_allocation(buffer->flags)) {
 		pr_err("%s: CMA heap doesn't support secure allocations\n",
 		       __func__);
-		return -EINVAL;
+		goto free_info;
 	}
 
 	if (align > CONFIG_CMA_ALIGNMENT)
 		align = CONFIG_CMA_ALIGNMENT;
 
-	pages = cma_alloc(cma_heap->cma, nr_pages, align, false);
-	if (!pages)
-		return -ENOMEM;
-
-	if (hlos_accessible_buffer(buffer)) {
-		if (PageHighMem(pages)) {
-			unsigned long nr_clear_pages = nr_pages;
-			struct page *page = pages;
-
-			while (nr_clear_pages > 0) {
-				void *vaddr = kmap_atomic(page);
+	if (!ion_cma_has_kernel_mapping(heap)) {
+		flags &= ~((unsigned long)ION_FLAG_CACHED);
+		buffer->flags = flags;
 
-				memset(vaddr, 0, PAGE_SIZE);
-				kunmap_atomic(vaddr);
-				page++;
-				nr_clear_pages--;
+		info->cpu_addr = dma_alloc_wc(dev, size, &info->handle,
+					      GFP_KERNEL);
+		if (!info->cpu_addr) {
+			dev_err(dev, "failed to allocate buffer\n");
+			goto free_info;
+		}
+		pages = pfn_to_page(PFN_DOWN(info->handle));
+	} else {
+		pages = cma_alloc(cma_heap->cma, nr_pages, align, false);
+		if (!pages)
+			goto free_info;
+
+		if (hlos_accessible_buffer(buffer)) {
+			if (PageHighMem(pages)) {
+				unsigned long nr_clear_pages = nr_pages;
+				struct page *page = pages;
+
+				while (nr_clear_pages > 0) {
+					void *vaddr = kmap_atomic(page);
+
+					memset(vaddr, 0, PAGE_SIZE);
+					kunmap_atomic(vaddr);
+					page++;
+					nr_clear_pages--;
+				}
+			} else {
+				memset(page_address(pages), 0, size);
 			}
-		} else {
-			memset(page_address(pages), 0, size);
 		}
-	}
 
-	if (MAKE_ION_ALLOC_DMA_READY ||
-	    (!hlos_accessible_buffer(buffer)) ||
-	     (!ion_buffer_cached(buffer)))
-		ion_pages_sync_for_device(dev, pages, size,
-					  DMA_BIDIRECTIONAL);
+		if (MAKE_ION_ALLOC_DMA_READY ||
+			(!hlos_accessible_buffer(buffer)) ||
+			(!ion_buffer_cached(buffer)))
+			ion_pages_sync_for_device(dev, pages, size,
+						DMA_BIDIRECTIONAL);
+	}
 
 	table = kmalloc(sizeof(*table), GFP_KERNEL);
 	if (!table)
-		goto err;
+		goto err_alloc;
 
 	ret = sg_alloc_table(table, 1, GFP_KERNEL);
 	if (ret)
-		goto free_mem;
+		goto free_table;
 
 	sg_set_page(table->sgl, pages, size, 0);
 
 	buffer->priv_virt = pages;
+	info->pages = pages;
+	buffer->priv_virt = info;
 	buffer->sg_table = table;
 	return 0;
 
-free_mem:
+free_table:
 	kfree(table);
-err:
-	cma_release(cma_heap->cma, pages, nr_pages);
+err_alloc:
+	if (info->cpu_addr)
+		dma_free_attrs(dev, size, info->cpu_addr, info->handle, 0);
+	else
+		cma_release(cma_heap->cma, pages, nr_pages);
+free_info:
+	kfree(info);
 	return -ENOMEM;
 }
 
 static void ion_cma_free(struct ion_buffer *buffer)
 {
 	struct ion_cma_heap *cma_heap = to_cma_heap(buffer->heap);
-	struct page *pages = buffer->priv_virt;
-	unsigned long nr_pages = PAGE_ALIGN(buffer->size) >> PAGE_SHIFT;
+	struct ion_cma_buffer_info *info = buffer->priv_virt;
+
+	if (info->cpu_addr) {
+		struct device *dev = buffer->heap->priv;
+
+		dma_free_attrs(dev, PAGE_ALIGN(buffer->size), info->cpu_addr,
+			       info->handle, 0);
+	} else {
+		struct page *pages = sg_page(buffer->sg_table->sgl);
+		unsigned long nr_pages = PAGE_ALIGN(buffer->size) >> PAGE_SHIFT;
 
-	/* release memory */
-	cma_release(cma_heap->cma, pages, nr_pages);
+		/* release memory */
+		cma_release(cma_heap->cma, pages, nr_pages);
+	}
 	/* release sg table */
 	sg_free_table(buffer->sg_table);
 	kfree(buffer->sg_table);
+	kfree(info);
 }
 
 static struct ion_heap_ops ion_cma_ops = {
@@ -129,9 +183,6 @@ struct ion_heap *ion_cma_heap_create(struct ion_platform_heap *data)
 	struct ion_cma_heap *cma_heap;
 	struct device *dev = (struct device *)data->priv;
 
-	if (!dev->cma_area)
-		return ERR_PTR(-EINVAL);
-
 	cma_heap = kzalloc(sizeof(*cma_heap), GFP_KERNEL);
 
 	if (!cma_heap)
@@ -225,9 +276,6 @@ struct ion_heap *ion_cma_secure_heap_create(struct ion_platform_heap *data)
 	struct ion_cma_heap *cma_heap;
 	struct device *dev = (struct device *)data->priv;
 
-	if (!dev->cma_area)
-		return ERR_PTR(-EINVAL);
-
 	cma_heap = kzalloc(sizeof(*cma_heap), GFP_KERNEL);
 
 	if (!cma_heap)
-- 
2.48.1

