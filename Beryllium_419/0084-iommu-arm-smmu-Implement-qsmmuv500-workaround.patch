From 0c0c14e2c152642e04e7480f618e0ac57b0719cd Mon Sep 17 00:00:00 2001
From: Pavel Dubrova <pashadubrova@gmail.com>
Date: Mon, 15 Nov 2021 23:04:55 +0200
Subject: [PATCH 084/175] iommu: arm-smmu: Implement qsmmuv500 workaround

While a tlb flush is in progress, clients may experience suboptimal
performance. To mitigate this, add a minimum delay between tlb
operations for real-time clients. This is implemented using a global
lock between HLOS and other execution enviroments.

Signed-off-by: Patrick Daly <pdaly@codeaurora.org>
Signed-off-by: Pavel Dubrova <pashadubrova@gmail.com>
---
 drivers/iommu/arm-smmu.c     | 190 +++++++++++++++++++++++++++++++++++
 include/trace/events/iommu.h |  14 +++
 2 files changed, 204 insertions(+)

diff --git a/drivers/iommu/arm-smmu.c b/drivers/iommu/arm-smmu.c
index 469649eb78e7..51fa7d753ab4 100644
--- a/drivers/iommu/arm-smmu.c
+++ b/drivers/iommu/arm-smmu.c
@@ -52,6 +52,7 @@
 #include <linux/platform_device.h>
 #include <linux/slab.h>
 #include <linux/spinlock.h>
+#include <linux/hwspinlock.h>
 #include <soc/qcom/secure_buffer.h>
 #include <linux/of_platform.h>
 #include <linux/msm-bus.h>
@@ -268,6 +269,7 @@ struct arm_smmu_device {
 #define ARM_SMMU_OPT_DISABLE_ATOS	(1 << 7)
 #define ARM_SMMU_OPT_NO_DYNAMIC_ASID	(1 << 8)
 #define ARM_SMMU_OPT_HALT		(1 << 9)
+#define ARM_SMMU_OPT_MMU500_ERRATA1	(1 << 10)
 	u32				options;
 	enum arm_smmu_arch_version	version;
 	enum arm_smmu_implementation	model;
@@ -401,6 +403,9 @@ struct arm_smmu_domain {
 	struct list_head		nonsecure_pool;
 	struct iommu_debug_attachment	*logger;
 	struct iommu_domain		domain;
+
+	bool				qsmmuv500_errata1_init;
+	bool				qsmmuv500_errata1_client;
 };
 
 struct arm_smmu_option_prop {
@@ -420,6 +425,10 @@ struct qsmmuv500_archdata {
 	struct actlr_setting		*actlrs;
 	u32				actlr_tbl_size;
 	u32				testbus_version;
+	struct arm_smmu_smr		*errata1_clients;
+	u32				num_errata1_clients;
+	struct hwspinlock 		*errata1_lock;
+	ktime_t				last_tlbi_ktime;
 };
 #define get_qsmmuv500_archdata(smmu)				\
 	((struct qsmmuv500_archdata *)(smmu->archdata))
@@ -455,6 +464,7 @@ static struct arm_smmu_option_prop arm_smmu_options[] = {
 	{ ARM_SMMU_OPT_DISABLE_ATOS, "qcom,disable-atos" },
 	{ ARM_SMMU_OPT_NO_DYNAMIC_ASID, "qcom,no-dynamic-asid" },
 	{ ARM_SMMU_OPT_HALT, "qcom,enable-smmu-halt"},
+	{ ARM_SMMU_OPT_MMU500_ERRATA1, "qcom,mmu500-errata-1" },
 	{ 0, NULL},
 };
 
@@ -477,6 +487,7 @@ static int arm_smmu_enable_s1_translations(struct arm_smmu_domain *smmu_domain);
 static int arm_smmu_alloc_cb(struct iommu_domain *domain,
 				struct arm_smmu_device *smmu,
 				struct device *dev);
+static struct iommu_gather_ops qsmmuv500_errata1_smmu_gather_ops;
 
 static bool arm_smmu_is_static_cb(struct arm_smmu_device *smmu);
 static bool arm_smmu_is_master_side_secure(struct arm_smmu_domain *smmu_domain);
@@ -2597,6 +2608,9 @@ static int arm_smmu_init_domain_context(struct iommu_domain *domain,
 		(smmu->model == QCOM_SMMUV500))
 		quirks |= IO_PGTABLE_QUIRK_QSMMUV500_NON_SHAREABLE;
 
+	if (smmu->options & ARM_SMMU_OPT_MMU500_ERRATA1)
+		smmu_domain->tlb_ops = &qsmmuv500_errata1_smmu_gather_ops;
+
 	if (arm_smmu_is_slave_side_secure(smmu_domain))
 		smmu_domain->tlb_ops = &msm_smmu_gather_ops;
 
@@ -6028,6 +6042,136 @@ static bool arm_smmu_fwspec_match_smr(struct iommu_fwspec *fwspec,
 	return false;
 }
 
+static bool qsmmuv500_errata1_required(struct arm_smmu_domain *smmu_domain,
+				 struct qsmmuv500_archdata *data)
+{
+	bool ret = false;
+	int j;
+	struct arm_smmu_smr *smr;
+	struct iommu_fwspec *fwspec;
+
+	if (smmu_domain->qsmmuv500_errata1_init)
+		return smmu_domain->qsmmuv500_errata1_client;
+
+	fwspec = smmu_domain->dev->iommu_fwspec;
+	for (j = 0; j < data->num_errata1_clients; j++) {
+		smr = &data->errata1_clients[j];
+		if (arm_smmu_fwspec_match_smr(fwspec, smr)) {
+			ret = true;
+			break;
+		}
+	}
+
+	smmu_domain->qsmmuv500_errata1_init = true;
+	smmu_domain->qsmmuv500_errata1_client = ret;
+	return ret;
+}
+
+#define SCM_CONFIG_ERRATA1_CLIENT_ALL 0x2
+#define SCM_CONFIG_ERRATA1 0x3
+static void __qsmmuv500_errata1_tlbiall(struct arm_smmu_domain *smmu_domain)
+{
+	struct arm_smmu_device *smmu = smmu_domain->smmu;
+	struct device *dev = smmu_domain->dev;
+	struct arm_smmu_cfg *cfg = &smmu_domain->cfg;
+	void __iomem *base;
+	int ret;
+	ktime_t cur;
+	u32 val;
+	struct scm_desc desc = {
+		.args[0] = SCM_CONFIG_ERRATA1_CLIENT_ALL,
+		.args[1] = false,
+		.arginfo = SCM_ARGS(2, SCM_VAL, SCM_VAL),
+	};
+
+	base = ARM_SMMU_CB(smmu, cfg->cbndx);
+	writel_relaxed(0, base + ARM_SMMU_CB_S1_TLBIALL);
+	writel_relaxed(0, base + ARM_SMMU_CB_TLBSYNC);
+	if (!readl_poll_timeout_atomic(base + ARM_SMMU_CB_TLBSTATUS, val,
+				      !(val & TLBSTATUS_SACTIVE), 0, 100))
+		return;
+
+	ret = scm_call2_atomic(SCM_SIP_FNID(SCM_SVC_SMMU_PROGRAM,
+					    SCM_CONFIG_ERRATA1),
+			       &desc);
+	if (ret) {
+		dev_err(smmu->dev, "Calling into TZ to disable ERRATA1 failed - IOMMU hardware in bad state\n");
+		BUG();
+		return;
+	}
+
+	cur = ktime_get();
+	trace_tlbi_throttle_start(dev, 0);
+	msm_bus_noc_throttle_wa(true);
+
+	if (readl_poll_timeout_atomic(base + ARM_SMMU_CB_TLBSTATUS, val,
+			      !(val & TLBSTATUS_SACTIVE), 0, 10000)) {
+		dev_err(smmu->dev, "ERRATA1 TLBSYNC timeout - IOMMU hardware in bad state");
+		trace_tlbsync_timeout(dev, 0);
+		BUG();
+	}
+
+	msm_bus_noc_throttle_wa(false);
+	trace_tlbi_throttle_end(dev, ktime_us_delta(ktime_get(), cur));
+
+	desc.args[1] = true;
+	ret = scm_call2_atomic(SCM_SIP_FNID(SCM_SVC_SMMU_PROGRAM,
+					    SCM_CONFIG_ERRATA1),
+			       &desc);
+	if (ret) {
+		dev_err(smmu->dev, "Calling into TZ to reenable ERRATA1 failed - IOMMU hardware in bad state\n");
+		BUG();
+	}
+}
+
+/* Must be called with clocks/regulators enabled */
+#define ERRATA1_TLBI_INTERVAL_US	10
+/* Timeout (ms) for the trylock of remote spinlocks */
+#define HWSPINLOCK_TIMEOUT	1000
+static void qsmmuv500_errata1_tlb_inv_context(void *cookie)
+{
+	struct arm_smmu_domain *smmu_domain = cookie;
+	struct device *dev = smmu_domain->dev;
+	struct qsmmuv500_archdata *data =
+			get_qsmmuv500_archdata(smmu_domain->smmu);
+	ktime_t cur;
+	unsigned long flags;
+	bool errata;
+	int ret;
+
+	cur = ktime_get();
+	trace_tlbi_start(dev, 0);
+
+	errata = qsmmuv500_errata1_required(smmu_domain, data);
+	ret = hwspin_lock_timeout_irqsave(data->errata1_lock,
+					    HWSPINLOCK_TIMEOUT, &flags);
+	if (ret)
+		WARN(1, "%s: get the hw lock failed", dev_name(dev));
+
+	if (errata) {
+		s64 delta;
+
+		delta = ktime_us_delta(ktime_get(), data->last_tlbi_ktime);
+		if (delta < ERRATA1_TLBI_INTERVAL_US)
+			udelay(ERRATA1_TLBI_INTERVAL_US - delta);
+
+		__qsmmuv500_errata1_tlbiall(smmu_domain);
+
+		data->last_tlbi_ktime = ktime_get();
+	} else {
+		__qsmmuv500_errata1_tlbiall(smmu_domain);
+	}
+	hwspin_unlock_irqrestore(data->errata1_lock, &flags);
+
+	trace_tlbi_end(dev, ktime_us_delta(ktime_get(), cur));
+}
+
+static struct iommu_gather_ops qsmmuv500_errata1_smmu_gather_ops = {
+	.tlb_flush_all	= qsmmuv500_errata1_tlb_inv_context,
+	.alloc_pages_exact = arm_smmu_alloc_pages_exact,
+	.free_pages_exact = arm_smmu_free_pages_exact,
+};
+
 static int qsmmuv500_tbu_halt(struct qsmmuv500_tbu_device *tbu,
 				struct arm_smmu_domain *smmu_domain)
 {
@@ -6460,6 +6604,48 @@ static int qsmmuv500_tbu_register(struct device *dev, void *cookie)
 	return 0;
 }
 
+static int qsmmuv500_parse_errata1(struct arm_smmu_device *smmu)
+{
+	int len, i, hwlock_id;
+	struct device *dev = smmu->dev;
+	struct qsmmuv500_archdata *data = get_qsmmuv500_archdata(smmu);
+	struct arm_smmu_smr *smrs;
+	const __be32 *cell;
+
+	cell = of_get_property(dev->of_node, "qcom,mmu500-errata-1", NULL);
+	if (!cell)
+		return 0;
+
+	hwlock_id = of_hwspin_lock_get_id(dev->of_node, 0);
+	if (hwlock_id < 0) {
+		if (hwlock_id != -EPROBE_DEFER)
+			dev_err(dev, "failed to retrieve hwlock\n");
+		return hwlock_id;
+	}
+
+	data->errata1_lock = hwspin_lock_request_specific(hwlock_id);
+	if (!data->errata1_lock)
+		return -ENXIO;
+
+	len = of_property_count_elems_of_size(
+			dev->of_node, "qcom,mmu500-errata-1", sizeof(u32) * 2);
+	if (len < 0)
+		return 0;
+
+	smrs = devm_kzalloc(dev, sizeof(*smrs) * len, GFP_KERNEL);
+	if (!smrs)
+		return -ENOMEM;
+
+	for (i = 0; i < len; i++) {
+		smrs[i].id = of_read_number(cell++, 1);
+		smrs[i].mask = of_read_number(cell++, 1);
+	}
+
+	data->errata1_clients = smrs;
+	data->num_errata1_clients = len;
+	return 0;
+}
+
 static int qsmmuv500_read_actlr_tbl(struct arm_smmu_device *smmu)
 {
 	int len, i;
@@ -7110,6 +7296,10 @@ static int qsmmuv500_arch_init(struct arm_smmu_device *smmu)
 	if (arm_smmu_is_static_cb(smmu))
 		return 0;
 
+	ret = qsmmuv500_parse_errata1(smmu);
+	if (ret)
+		return ret;
+
 	ret = qsmmuv500_read_actlr_tbl(smmu);
 	if (ret)
 		return ret;
diff --git a/include/trace/events/iommu.h b/include/trace/events/iommu.h
index 0db6f7f86270..dea920328a70 100644
--- a/include/trace/events/iommu.h
+++ b/include/trace/events/iommu.h
@@ -243,6 +243,20 @@ DEFINE_EVENT(iommu_tlbi, tlbsync_timeout,
 	TP_ARGS(dev, time)
 );
 
+DEFINE_EVENT(iommu_tlbi, tlbi_throttle_start,
+
+	TP_PROTO(struct device *dev, u64 time),
+
+	TP_ARGS(dev, time)
+);
+
+DEFINE_EVENT(iommu_tlbi, tlbi_throttle_end,
+
+	TP_PROTO(struct device *dev, u64 time),
+
+	TP_ARGS(dev, time)
+);
+
 TRACE_EVENT(smmu_init,
 
 	TP_PROTO(u64 time),
-- 
2.48.1

